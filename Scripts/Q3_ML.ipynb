{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b38524",
   "metadata": {},
   "source": [
    "Can we predict the severity of an accident based on time, weather, and location data?\n",
    "    Using Random Forest, Logistic Regression, and SVM (multiclass classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b7e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f7931",
   "metadata": {},
   "source": [
    "###Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6aa201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsample size for experiments: 90374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Severity', 'Start_Time', 'End_Time', 'Lat', 'Lng', 'Street',\n",
       "       'City', 'County', 'State', 'Wind_Chill(F)', 'Humidity(%)',\n",
       "       'Pressure(in)', 'Sunrise_Sunset', 'Severity_Label', 'Is_Day',\n",
       "       'Temperature(F)', 'Visibility(mi)', 'Wind_Speed(mph)',\n",
       "       'Precipitation(in)', 'Weather_Condition', 'Weather_Simple', 'Hour',\n",
       "       'Day_of_Week', 'Month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "#dataset\n",
    "df = pd.read_csv('../Data/us_accidents_sample_500k_clean.csv')\n",
    "\n",
    "#target \n",
    "#df.Severity.value_counts() #already int\n",
    "\n",
    "# Extract time features from Start_Time\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "df['Hour'] = df['Start_Time'].dt.hour\n",
    "df['Day_of_Week'] = df['Start_Time'].dt.dayofweek\n",
    "df['Month'] = df['Start_Time'].dt.month\n",
    "\n",
    "# Reduce high-cardinality by grouping rare categories for 'State' and 'Weather_Simple'\n",
    "state_counts = df['State'].value_counts()\n",
    "rare_states = state_counts[state_counts < 500].index  # threshold: group states with <500 occurrences\n",
    "df['State'] = df['State'].replace(rare_states, 'Other_State')\n",
    "\n",
    "weather_counts = df['Weather_Simple'].value_counts()\n",
    "rare_weather = weather_counts[weather_counts < 1000].index\n",
    "df['Weather_Simple'] = df['Weather_Simple'].replace(rare_weather, 'Other_Weather')\n",
    "\n",
    "# Small stratified subsample for fast experiments (optional)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.2, random_state=42)\n",
    "train_idx, _ = next(sss.split(df, df['Severity']))\n",
    "df_sample = df.iloc[train_idx].copy()\n",
    "print('Subsample size for experiments:', len(df_sample))\n",
    "\n",
    "# show columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c761229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified subsample created: df_sample with 90374 rows\n"
     ]
    }
   ],
   "source": [
    "# Stratified subsample for faster experiments (20% of data, preserves class balance)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.2, random_state=42)\n",
    "sample_idx, _ = next(sss.split(df, df['Severity']))\n",
    "df_sample = df.iloc[sample_idx].copy()\n",
    "print('Stratified subsample created: df_sample with', len(df_sample), 'rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e527b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and testing\n",
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=df.Severity  #4 classes balanced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328cf34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values per feature:\n",
      "Weather_Simple: 0\n",
      "Visibility(mi): 10185\n",
      "Sunrise_Sunset: 1166\n",
      "State: 0\n",
      "Hour: 0\n",
      "Day_of_Week: 0\n",
      "Month: 0\n",
      "Precipitation(in): 0\n",
      "Humidity(%): 10016\n",
      "Wind_Speed(mph): 0\n",
      "\n",
      "Training set size after dropping NAs: 350888\n",
      "Test set size after dropping NAs: 87662\n"
     ]
    }
   ],
   "source": [
    "#chose feature + target\n",
    "features = ['Weather_Simple', 'Visibility(mi)', 'Sunrise_Sunset', 'State', \n",
    "            'Hour', 'Day_of_Week', 'Month', 'Precipitation(in)', 'Humidity(%)', 'Wind_Speed(mph)']\n",
    "target = 'Severity'\n",
    "\n",
    "# check null values for new features\n",
    "print(\"Null values per feature:\")\n",
    "for col in features:\n",
    "    print(f\"{col}: {df[col].isnull().sum()}\")\n",
    "\n",
    "# create stratified train/test split\n",
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=df.Severity  #4 classes balanced\n",
    ")\n",
    "\n",
    "# For ML: drop rows with missing values in features (after split to avoid leakage)\n",
    "train = train.dropna(subset=features + ['Severity'])\n",
    "test = test.dropna(subset=features + ['Severity'])\n",
    "print(f\"\\nTraining set size after dropping NAs: {len(train)}\")\n",
    "print(f\"Test set size after dropping NAs: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb54e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training and testing subsets\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8920248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing(OneHot and num combo)\n",
    "\n",
    "# numeric pipeline: impute then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "num_features = ['Visibility(mi)', 'Precipitation(in)', 'Humidity(%)', 'Wind_Speed(mph)', 'Hour', 'Day_of_Week', 'Month']\n",
    "cat_features = ['Weather_Simple', 'Sunrise_Sunset', 'State']\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features),\n",
    "        ('num', numeric_transformer, num_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7c6e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "rand_forest = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', RandomForestClassifier(n_estimators=100, max_depth=12, max_features='sqrt', n_jobs=-1, random_state=42))\n",
    "]) \n",
    "#n_jobs=-1 uses more power to make it run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d75927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Score: [0.77791267 0.77768183 0.77787658]\n",
      "Mean Cross-Validation accuracy: 0.777823693180964\n"
     ]
    }
   ],
   "source": [
    "#cross-validation only for Training\n",
    "scores = cross_val_score(rand_forest, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "print(\"Cross-Validation Score:\", scores)\n",
    "print(\"Mean Cross-Validation accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978ad92",
   "metadata": {},
   "source": [
    "###logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "306a11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 361499\n",
      "Test set size: 90375\n"
     ]
    }
   ],
   "source": [
    "#logistic regression multiclass \n",
    "#split data \n",
    "y = df[target] #target\n",
    "X = df[features] #Features\n",
    "#train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y           #stratify by the target == severity\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#drop rows with NAs\n",
    "train_data = pd.concat([X_train, y_train], axis=1).dropna(subset=features + [target])\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "\n",
    "test_data = pd.concat([X_test, y_test], axis=1).dropna(subset=features + [target])\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "830efe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "# reuse numeric_transformer defined earlier\n",
    "cat_cols = ['Weather_Simple', 'Sunrise_Sunset', 'State']\n",
    "num_cols = ['Visibility(mi)', 'Precipitation(in)', 'Humidity(%)', 'Wind_Speed(mph)', 'Hour', 'Day_of_Week', 'Month']\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features),\n",
    "        ('num', numeric_transformer, num_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a0a667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "log_model = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "32203ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation scores: [0.77740824 0.77718595 0.77742344]\n",
      "Mean Cross-Validation accuracy: 0.7773392081924388\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "scores = cross_val_score(\n",
    "    log_model, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Cross-Validation scores:\", scores)\n",
    "print(\"Mean Cross-Validation accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69914ff3",
   "metadata": {},
   "source": [
    "###SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Use the same split from logistic regression\n",
    "train_data_svm = pd.concat([X_train, y_train], axis=1).dropna(subset=features + [target])\n",
    "X_train_svm = train_data_svm[features]\n",
    "y_train_svm = train_data_svm[target]\n",
    "\n",
    "test_data_svm = pd.concat([X_test, y_test], axis=1).dropna(subset=features + [target])\n",
    "X_test_svm = test_data_svm[features]\n",
    "y_test_svm = test_data_svm[target]\n",
    "\n",
    "# Pipeline for SVM\n",
    "svm_model = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', SVC(kernel='linear', random_state=42, probability=True))\n",
    "])\n",
    "\n",
    "# Cross validation (fewer folds for speed, parallelize)\n",
    "scores_svm = cross_val_score(svm_model, X_train_svm, y_train_svm, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"SVM Cross-Validation scores:\", scores_svm)\n",
    "print(\"Mean SVM Cross-Validation accuracy:\", scores_svm.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f5914",
   "metadata": {},
   "source": [
    "###Best Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6277643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the best ML model\n",
    "#hyperparam tuning\n",
    "\n",
    "#refit final model\n",
    "\n",
    "#final Eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
